{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary, where the original form stays at it is and the normalized form is: POS_LEMMA. For example\n",
    "est, VER_estre\n",
    "les, ART_le\n",
    "\n",
    "The POS tagging and lemmatization is here perfermed with TreeTaggerWrapper (a TreeTagger Python version), using the parameters for Old French made available by Achim Stein.\n",
    "\n",
    "## N.b.: change the parameter below to have the dictionary for one particular example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for POS tagging and lemma annotation, plus some cleaning, of the text of all witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import treetaggerwrapper, os, re\n",
    "\n",
    "\n",
    "def create_poslemma(example): \n",
    "\n",
    "\n",
    "    # create a folder where to store intermediate results.\n",
    "    try:\n",
    "        os.makedirs('data/' + example + '/process')\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # put together the texts of all the witnesses\n",
    "    with open('data/' + example + '/W1.txt') as W1, \\\n",
    "         open('data/' + example + '/W2.txt') as W2, \\\n",
    "         open('data/' + example + '/W3.txt') as W3, \\\n",
    "         open('data/' + example + '/W4.txt') as W4, \\\n",
    "         open('data/' + example + '/process/p1_all_texts.txt', 'w', encoding='utf-8') as all_texts:\n",
    "        all_texts.write(W1.read() + ' ' + W2.read() + ' ' + W3.read() + ' ' + W4.read())\n",
    "\n",
    "    # each word a new line, in alphabetical order: sorted()\n",
    "    infile = open('data/' + example + '/process/p1_all_texts.txt').read()\n",
    "    outfile = open('data/' + example + '/process/p2_all_words.txt', 'w', encoding='utf-8')\n",
    "    words = infile.split()\n",
    "    for word in sorted(words):\n",
    "        outfile.write(word + '\\n')\n",
    "\n",
    "    # delete duplicates\n",
    "    infile = 'data/' + example + '/process/p2_all_words.txt'\n",
    "    lines_seen = set() # holds lines already seen\n",
    "    outfile = open('data/' + example + '/process/p3_single_words.txt', 'w', encoding='utf-8')\n",
    "    for line in open(infile, \"r\"):\n",
    "        if line not in lines_seen: # not a duplicate\n",
    "            outfile.write(line)\n",
    "            lines_seen.add(line)\n",
    "    outfile.close()\n",
    "\n",
    "    # pos and lemma tagging\n",
    "    infile = 'data/' + example + '/process/p3_single_words.txt'\n",
    "    outfile = 'data/' + example + '/process/p4_single_words_analyzed.txt'\n",
    "    open(outfile, 'w', encoding='utf-8')\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='stein')\n",
    "    tagger.tag_file_to(infile, outfile)\n",
    "\n",
    "\n",
    "    # clean pos and lemma tagging, keeping only the first lemma suggested\n",
    "    infile = 'data/' + example + '/process/p4_single_words_analyzed.txt'\n",
    "    outfile = 'data/' + example + '/process/p5_single_words_analyzed_clean.txt'\n",
    "    patterns = [('_.*', ''),\n",
    "                ('\\d.*', ''),\n",
    "    ##            ('\\|.*', ''), so that the different output possibilities are saved\n",
    "                ('�', 'ö'),  ## encoding problem, but it does seem to depend on the TreeTaggerWrapper, nor on the script. Maybe on the lexicon? Anyway, this is not real solution but works\n",
    "                ('<nolem>', 'UNKNOWN')]\n",
    "    t = open(infile).read()\n",
    "    for (p1,p2) in patterns:\n",
    "        p = re.compile(p1)\n",
    "        t = p.sub(p2, t)\n",
    "    o = open(outfile, 'w', encoding='utf-8')\n",
    "    o.write(t)\n",
    "    o.close()\n",
    "\n",
    "\n",
    "    # create pos_lemma.csv\n",
    "    infile = open('data/' + example + '/process/p5_single_words_analyzed_clean.txt', 'r')\n",
    "    outfile = open('pos_lemma_' + example + '.csv', 'w', encoding='utf-8')\n",
    "    for aline in infile:\n",
    "        values = aline.split()\n",
    "        original = values[0]\n",
    "        pos = values[1]\n",
    "        lemmas = values[2]\n",
    "        print(original + ',' + pos + '_' + lemmas + '\\n')   ## print them, in order to have immediately check\n",
    "        outfile.write(original + ',' + pos + '_' + lemmas + '\\n')   ## print them on file\n",
    "    infile.close()\n",
    "    outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function with the number of the example as parameter (ex. 'example1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a,VER_avoir\n",
      "\n",
      "achever,VER_achever\n",
      "\n",
      "ait,VER_aidier|avoir\n",
      "\n",
      "autres,ADJ_autre\n",
      "\n",
      "aventures,NOM_aventure\n",
      "\n",
      "car,CON:coord_car\n",
      "\n",
      "chevalerie,NOM_chevalerie\n",
      "\n",
      "chevalier,NOM_chevalier\n",
      "\n",
      "dex,NOM_dieu\n",
      "\n",
      "diex,NOM_dieu\n",
      "\n",
      "en,PRO:clit_en\n",
      "\n",
      "la,PRO:pers_ele|il\n",
      "\n",
      "lairoiz,VER_laissier\n",
      "\n",
      "lerroiz,VER_lerroiz\n",
      "\n",
      "les,DET:def_le\n",
      "\n",
      "lesroiz,NOM_lesroiz\n",
      "\n",
      "mis,VER_manoir|metre|mettre\n",
      "\n",
      "mise,VER_metre|mettre\n",
      "\n",
      "ou,PROCON_o|oö\n",
      "\n",
      "par,PRE_par\n",
      "\n",
      "proece,NOM_pröece\n",
      "\n",
      "proesce,NOM_pröece\n",
      "\n",
      "quar,CON:coord_car\n",
      "\n",
      "que,PROCON_que\n",
      "\n",
      "vos,PRO:pers_ous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_poslemma('example2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
